{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenizer \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "s_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Create listFileName for all the files in diabetes corpus\n",
    "listFileName = [fileName for fileName in os.listdir(os.getcwd()+\"/diabetes/\") if \".txt\" in str(fileName)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build topic modeling for individual documents in the corpus\n",
    "texts = []\n",
    "topicsAppend = []\n",
    "globalWordRaw = []\n",
    "globalWordList = []\n",
    "pertopic = []\n",
    "listwords = []\n",
    "index = 0\n",
    "for file in listFileName:    \n",
    "    filepath = open(os.getcwd()+\"/diabetes/\"+file)\n",
    "    i = filepath.read()\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [s_stemmer.stem(i) for i in stopped_tokens]    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "    filepath.close()\n",
    "    # start topic modelling process\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=1)\n",
    "    topicsAppend.append(ldamodel.print_topics(num_topics=1, num_words=30))\n",
    "    tmplistwords = re.findall(r'\\\"[\\w+]+\\\"',topicsAppend[index][0][1])\n",
    "    index = index + 1\n",
    "    for x in listwords:\n",
    "        pertopic.append(x)\n",
    "    globalWordRaw.append(\" \".join(pertopic))\n",
    "    listwords = [i.replace('\"', '') for i in tmplistwords]\n",
    "    globalWordList.append(listwords)\n",
    "    texts = []\n",
    "    pertopic = []\n",
    "    listwords = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building the corpus topic modelling \n",
    "texts = []\n",
    "for file in listFileName:    \n",
    "    filepath = open(os.getcwd()+\"/diabetes/\"+file)\n",
    "    i = filepath.read()\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]  \n",
    "    stemmed_tokens = [s_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts.append(stemmed_tokens)\n",
    "    filepath.close()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=1)\n",
    "tmpglobalTopic = re.findall(r'\\\"[\\w+]+\\\"',ldamodel.print_topics(num_topics=1, num_words=30)[0][1])\n",
    "globalTopic = [i.replace('\"', '') for i in tmpglobalTopic]\n",
    "print(ldamodel.print_topics(num_topics=1, num_words=30))\n",
    "\n",
    "#Build feature set for our model\n",
    "globalVector = []\n",
    "globalTopicVector = []\n",
    "for g2 in globalWordList:\n",
    "    common = set(globalTopic).intersection(g2)\n",
    "    common_new = list(common)\n",
    "    tmpVector = [] \n",
    "    for x in globalTopic:\n",
    "        try: \n",
    "            common_new.index(x) \n",
    "            tmpVector.append(1) \n",
    "        except: \n",
    "            tmpVector.append(0)\n",
    "    globalVector.append(tmpVector)\n",
    "    globalTopicVector.append(len(common_new))\n",
    "    tmpVector = []\n",
    "\n",
    "\n",
    "numofDiabetes = []\n",
    "numofShockingWords = []\n",
    "shockSum = 0\n",
    "shockingWords = [\"surpris\", \"astonish\", \"howev\", \"unexpect\", \"amaz\", \"shock\", \"astound\"]\n",
    "for myFile in listFileName:\n",
    "    filepath = open(os.getcwd()+\"/diabetes/\"+myFile)\n",
    "    i = filepath.read()\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop] \n",
    "    s_stemmed_tokens = [s_stemmer.stem(i) for i in stopped_tokens]\n",
    "    fdist = FreqDist(s_stemmed_tokens)\n",
    "    filepath.close()\n",
    "    numofDiabetes.append(fdist[\"diabet\"])\n",
    "    for word in shockingWords:\n",
    "        shockSum = shockSum + fdist[word]\n",
    "    numofShockingWords.append(shockSum)\n",
    "    shockSum = 0\n",
    "\n",
    "\n",
    "#Build Model\n",
    "mydf = pd.DataFrame(globalVector, columns=globalTopic, index=listFileName)\n",
    "traindf = mydf\n",
    "\n",
    "#Get label information from 2 users\n",
    "labelsUser1df = pd.read_csv(\"fileName.csv\", index_col=\"Filename\")\n",
    "labelsUser2df = pd.read_csv(\"supriseMe.csv\", index_col=\"FileName\")\n",
    "\n",
    "#Train the model\n",
    "trainclf = svm.SVC()\n",
    "trainclf.fit(traindf[0:99], labelsUser2df[\"SupriseMe\"][0:99])\n",
    "trainforest = RandomForestClassifier(n_estimators = 100) \n",
    "trainforest = trainforest.fit( traindf[0:99], labelsUser2df[\"SupriseMe\"][0:99] )\n",
    "\n",
    "#Predict the model\n",
    "resultforest_train = trainforest.predict(traindf[100:1000])\n",
    "resultsvm_train = trainclf.predict(traindf[100:1000])\n",
    "\n",
    "#Display the results of the suprising documents\n",
    "resultdf = pd.DataFrame(index=traindf[100:1000].index.tolist())\n",
    "resultdf = resultdf.assign(resultforest_train = resultforest_train)\n",
    "resultdf = resultdf.assign(resultsvm_train = resultsvm_train)\n",
    "pd.options.display.max_rows = 990\n",
    "pd.options.display.max_columns = 40\n",
    "resultdf[resultdf[\"resultforest_train\"] == \"1\"]\n",
    "resultdf[resultdf[\"resultsvm_train\"] == \"1\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
